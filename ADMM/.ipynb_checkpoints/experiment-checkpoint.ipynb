{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Codes adapted from: https://github.com/PotatoThanh/ADMM-NeuralNetworks/tree/master/python\n",
    "\n",
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "class ADMM_NN(object):\n",
    "    def __init__(self, n_inputs, n_hiddens, n_outputs, n_batches):\n",
    "        \"\"\"\n",
    "        Initialize variables for NN.\n",
    "        Not sure how initialization affects the performance.\n",
    "        The data should be in columns.\n",
    "        for example, the input size of MNIST data should be (28x28, *) instead of (*, 28x28).\n",
    "        Ignored bias terms.\n",
    "        :param n_inputs: Number of inputs.\n",
    "        :param n_hiddens: Number of hidden units.\n",
    "        :param n_outputs: Number of outputs\n",
    "        :param n_batches: Number of data sample that you want to train\n",
    "        \"\"\"\n",
    "        self.a0 = np.zeros((n_inputs, n_batches))\n",
    "\n",
    "        self.w1 = np.zeros((n_hiddens, n_inputs))\n",
    "        self.w2 = np.zeros((n_hiddens, n_hiddens))\n",
    "        self.w3 = np.zeros((n_outputs, n_hiddens))\n",
    "\n",
    "        self.z1 = np.random.rand(n_hiddens, n_batches)\n",
    "        self.a1 = np.random.rand(n_hiddens, n_batches)\n",
    "\n",
    "        self.z2 = np.random.rand(n_hiddens, n_batches)\n",
    "        self.a2 = np.random.rand(n_hiddens, n_batches)\n",
    "\n",
    "        self.z3 = np.random.rand(n_outputs, n_batches)\n",
    "\n",
    "        self.lambda_lagrange = np.ones((n_outputs, n_batches))\n",
    "\n",
    "    def _relu(self, x):\n",
    "        return tf.maximum(0.0, x)\n",
    "\n",
    "    def _weight_update(self, layer_output, activation_input):\n",
    "        \"\"\"\n",
    "        Consider it now the minimization of the problem with respect to W_l.\n",
    "        For each layer l, the optimal solution minimizes ||z_l - W_l a_l-1||^2. This is simply\n",
    "        a least square problem, and the solution is given by W_l = z_l p_l-1, where p_l-1\n",
    "        represents the pseudo-inverse of the rectangular activation matrix a_l-1.\n",
    "        :param layer_output: output matrix (z_l)\n",
    "        :param activation_input: activation matrix l-1  (a_l-1)\n",
    "        :return: weight matrix\n",
    "        \"\"\"\n",
    "        pinv = np.linalg.pinv(activation_input)\n",
    "        weight_matrix = tf.matmul(tf.cast(layer_output, tf.float32), tf.cast(pinv, tf.float32))\n",
    "        return weight_matrix \n",
    "\n",
    "    def _activation_update(self, next_weight, next_layer_output, layer_nl_output, beta, gamma):\n",
    "        \"\"\"\n",
    "        Minimization for a_l is a simple least squares problem similar to the weight update.\n",
    "        However, in this case the matrix appears in two penalty terms in the problem, and so\n",
    "        we must minimize:\n",
    "            beta ||z_l+1 - W_l+1 a_l||^2 + gamma ||a_l - h(z_l)||^2\n",
    "        :param next_weight:  weight matrix l+1 (w_l+1)\n",
    "        :param next_layer_output: output matrix l+1 (z_l+1)\n",
    "        :param layer_nl_output: activate output matrix h(z) (h(z_l))\n",
    "        :param beta: value of beta\n",
    "        :param gamma: value of gamma\n",
    "        :return: activation matrix\n",
    "        \"\"\"\n",
    "        layer_nl_output = self._relu(layer_nl_output)\n",
    "        \n",
    "        next_weight = tf.cast(next_weight, tf.float64)\n",
    "        m1 = beta*tf.matmul(tf.cast(tf.matrix_transpose(next_weight), tf.float64), next_weight)\n",
    "        m2 = tf.scalar_mul(gamma, tf.eye(tf.cast(m1.get_shape()[0], tf.int32)))\n",
    "        av = tf.matrix_inverse(tf.cast(m1, tf.float32) + tf.cast(m2, tf.float32))\n",
    "\n",
    "        m3 = beta*tf.matmul(tf.matrix_transpose(next_weight), tf.cast(next_layer_output, tf.float64))\n",
    "        m4 = gamma*layer_nl_output \n",
    "        af = tf.cast(m3, tf.float32) + tf.cast(m4, tf.float32)\n",
    "\n",
    "        return tf.matmul(av, af)\n",
    "\n",
    "\n",
    "    def _argminz(self, a, w, a_in, z_in, beta, gamma):\n",
    "        \"\"\"\n",
    "        This problem is non-convex and non-quadratic (because of the non-linear term h).\n",
    "        Fortunately, because the non-linearity h works entry-wise on its argument, the entries\n",
    "        in z_l are decoupled. This is particularly easy when h is piecewise linear, as it can\n",
    "        be solved in closed form; common piecewise linear choices for h include rectified\n",
    "        linear units (ReLUs), that is used here, and non-differentiable sigmoid functions.\n",
    "        :param a: activation matrix (a_l)\n",
    "        :param w:  weight matrix (w_l)\n",
    "        :param a_in: activation matrix l-1 (a_l-1)\n",
    "        :param beta: value of beta\n",
    "        :param gamma: value of gamma\n",
    "        :param z_in: z_l (matrix)\n",
    "        :return: output matrix\n",
    "        \"\"\"\n",
    "        m = tf.matmul(tf.cast(w, tf.float32), tf.cast(a_in, tf.float32))\n",
    "        #note that z can be either postive or negative,\n",
    "        #we need to compute for both possibilities and take the min\n",
    "        sol1 = (gamma*a + beta*m) / (gamma + beta)  #if z>=0\n",
    "        sol2 = m  #if z<0 \n",
    "        \n",
    "        sol1 = np.array(sol1)\n",
    "        sol2 = np.array(sol2)\n",
    "        z_in = np.array(z_in)\n",
    "        z = np.zeros_like(z_in)\n",
    "        \n",
    "        z[z_in>=0.] = sol1[z_in>=0.]\n",
    "        z[z_in<0.] = sol2[z_in<0.]\n",
    "\n",
    "        return z \n",
    "\n",
    "    def _argminlastz(self, targets, eps, w, a_in, beta):\n",
    "        \"\"\"\n",
    "        Minimization of the last output matrix.\n",
    "        Using square error as loss term here.\n",
    "        Treat lagrange as an element-wise product and find min of quadratic function.\n",
    "        target(y), lambda, z_L, all same dimension\n",
    "        :param targets: target matrix (equal dimensions of z) (y)\n",
    "        :param eps: lagrange multiplier matrix (equal dimensions of z) (lambda)\n",
    "        :param w: weight matrix (w_l)\n",
    "        :param a_in: activation matrix l-1 (a_l-1)\n",
    "        :param beta: value of beta\n",
    "        :return: output matrix last layer\n",
    "        \"\"\"\n",
    "        m = tf.matmul(tf.cast(w, tf.float32), tf.cast(a_in, tf.float32))\n",
    "        z = (targets - eps/2 +beta*m) / (1+beta)\n",
    "        return z \n",
    "\n",
    "    def _lambda_update(self, zl, w, a_in, beta):\n",
    "        \"\"\"\n",
    "        Lagrange multiplier update.\n",
    "        :param zl: output matrix last layer (z_L)\n",
    "        :param w: weight matrix last layer (w_L)\n",
    "        :param a_in: activation matrix l-1 (a_L-1)\n",
    "        :param beta: value of beta\n",
    "        :return: lagrange update\n",
    "        \"\"\"\n",
    "        mpt = tf.matmul(tf.cast(w, tf.float32), tf.cast(a_in, tf.float32))\n",
    "        lambda_up = beta*(zl - mpt)\n",
    "\n",
    "        return self.lambda_lagrange + lambda_up \n",
    "\n",
    "    def feed_forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Calculate feed forward pass for neural network\n",
    "        :param inputs: inputs features\n",
    "        :return: value of prediction\n",
    "        \"\"\"\n",
    "        outputs = self._relu(tf.matmul(self.w1, inputs))\n",
    "        outputs = self._relu(tf.matmul(self.w2, outputs))\n",
    "        # no activation for final layer\n",
    "        outputs = tf.matmul(self.w3, outputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, inputs, labels, beta, gamma):\n",
    "        \"\"\"\n",
    "        Training ADMM Neural Network by minimizing sub-problems\n",
    "        :param inputs: input of training data samples\n",
    "        :param outputs: label of training data samples\n",
    "        :param epochs: number of epochs\n",
    "        :param beta: value of beta\n",
    "        :param gamma: value of gamma\n",
    "        :return: loss value\n",
    "        \"\"\"\n",
    "        self.a0 = inputs \n",
    "\n",
    "        # Input layer \n",
    "        self.w1 = self._weight_update(self.z1, self.a0)\n",
    "        self.a1 = self._activation_update(self.w2, self.z2, self.z1, beta, gamma)\n",
    "        self.z1 = self._argminz(self.a1, self.w1, self.a0, self.z1, beta, gamma) \n",
    "\n",
    "        # Hidden layer (use loop if many layers)\n",
    "        self.w2 = self._weight_update(self.z2, self.a1)\n",
    "        self.a2 = self._activation_update(self.w3, self.z3, self.z2, beta, gamma)\n",
    "        self.z2 = self._argminz(self.a2, self.w2, self.a1, self.z2, beta, gamma)\n",
    "\n",
    "        # Output layer \n",
    "        self.w3 = self._weight_update(self.z3, self.a2)\n",
    "        self.z3 = self._argminlastz(labels, self.lambda_lagrange, self.w3, self.a2, beta)\n",
    "        self.lambda_lagrange = self._lambda_update(self.z3, self.w3, self.a2, beta)\n",
    "\n",
    "        loss, accuracy = self.evaluate(inputs, labels)\n",
    "        return loss, accuracy \n",
    "\n",
    "\n",
    "    def evaluate(self, inputs, labels, isCategories=True):\n",
    "        \"\"\"\n",
    "        Calculate loss and accuracy (only classification)\n",
    "        :param inputs: inputs data\n",
    "        :param outputs: ground truth\n",
    "        :param isCategrories: classification or not\n",
    "        :return: loss and accuracy (only classification)\n",
    "        \"\"\"\n",
    "        forward = self.feed_forward(inputs)\n",
    "        loss = tf.reduce_mean(tf.square(forward - labels))\n",
    "\n",
    "        if isCategories:\n",
    "            accuracy = tf.equal(tf.argmax(labels, axis=0), tf.argmax(forward, axis=0))\n",
    "            accuracy = tf.reduce_sum(tf.cast(accuracy, tf.int32)) / accuracy.get_shape()[0]\n",
    "        else:\n",
    "            # for regression, no so-called accuracy\n",
    "            accuracy = loss \n",
    "\n",
    "        return loss, accuracy\n",
    "\n",
    "    def warming(self, inputs, labels, epochs, beta, gamma):\n",
    "        \"\"\"\n",
    "        Warming ADMM Neural Network by minimizing sub-problems without update lambda for several iterations\n",
    "        :param inputs: input of training data samples\n",
    "        :param outputs: label of training data samples\n",
    "        :param epochs: number of epochs\n",
    "        :param beta: value of beta\n",
    "        :param gamma: value of gamma\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.a0 = inputs \n",
    "        for i in range(epochs):\n",
    "            print(\"------ Warming: {:d} ------\".format(i))\n",
    "\n",
    "            #Input layer \n",
    "            self.w1 = self._weight_update(self.z1, self.a0)\n",
    "            self.a1 = self._activation_update(self.w2, self.z2, self.z1, beta, gamma)\n",
    "            self.z1 = self._argminz(self.a1, self.w1, self.a0, self.z1, beta, gamma) \n",
    "\n",
    "            #Hidden layer\n",
    "            self.w2 = self._weight_update(self.z2, self.a1)\n",
    "            self.a2 = self._activation_update(self.w3, self.z3, self.z2, beta, gamma)\n",
    "            self.z2 = self._argminz(self.a2, self.w2, self.a1, self.z2, beta, gamma)\n",
    "\n",
    "            # Output layer\n",
    "            self.w3 = self._weight_update(self.z3, self.a2)\n",
    "            self.z3 = self._argminlastz(labels, self.lambda_lagrange, self.w3, self.a2, beta)\n",
    "\n",
    "\n",
    "    def drawcurve(self, train_, valid_, legend_1, legend_2):\n",
    "        acc_train = np.array(train_).flatten()\n",
    "        acc_test = np.array(valid_).flatten()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(acc_train)\n",
    "        plt.plot(acc_test)\n",
    "\n",
    "        plt.legend([legend_1, legend_2], loc=\"upper left\")\n",
    "        plt.draw()\n",
    "        plt.pause(0.001) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Warming: 0 ------\n",
      "------ Warming: 1 ------\n",
      "------ Warming: 2 ------\n",
      "------ Warming: 3 ------\n",
      "------ Warming: 4 ------\n",
      "------ Training: 0 ------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'isCategrories' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-9fceaaefc07a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"------ Training: {:d} ------\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mloss_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-13927427fd14>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, labels, beta, gamma)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambda_lagrange\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lambda_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-13927427fd14>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, inputs, labels, isCategories)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misCategrories\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m             \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'isCategrories' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow.contrib.eager as tfe\n",
    "#from ADMM import ADMM_NN\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#tfe.enable_eager_execution()\n",
    "\n",
    "# mnist = input_data.read_data_sets(\"./data/\", one_hot=True)\n",
    "\n",
    "# trainX = np.transpose(mnist.train.images).astype(np.float32)\n",
    "# trainY = np.transpose(mnist.train.labels).astype(np.float32)\n",
    "\n",
    "# validX = np.transpose(mnist.validation.images).astype(np.float32)\n",
    "# validY = np.transpose(mnist.validation.labels).astype(np.float32)\n",
    "\n",
    "# testX = np.transpose(mnist.test.images).astype(np.float32)\n",
    "# testY = np.transpose(mnist.test.labels).astype(np.float32)\n",
    "\n",
    "# # parameters \n",
    "# n_inputs = 28*28\n",
    "# n_outputs = 10\n",
    "# n_hiddens = 256 \n",
    "# n_batches = np.shape(trainX)[1]\n",
    "# train_epochs = 15 \n",
    "# warm_epochs = 5 \n",
    "\n",
    "# # these parameters are used in the paper \n",
    "# # theoretically can converge with any hyperparameters\n",
    "# # can tune these to check the effect\n",
    "# beta = 1.0\n",
    "# gamma = 10.0\n",
    "\n",
    "model = ADMM_NN(n_inputs, n_hiddens, n_outputs, n_batches)\n",
    "\n",
    "model.warming(trainX, trainY, warm_epochs, beta, gamma)\n",
    "\n",
    "list_loss_train = []\n",
    "list_loss_valid = []\n",
    "list_accuracy_train = []\n",
    "list_accuracy_valid = []\n",
    "\n",
    "for i in range(train_epochs):\n",
    "    print(\"------ Training: {:d} ------\".format(i))\n",
    "    loss_train, accuracy_train = model.fit(trainX, trainY, beta, gamma)\n",
    "    loss_valid, accuracy_valid = model.evaluate(validX, validY)\n",
    "\n",
    "    print(\"Loss train: %3f, accuracy train: %3f\" % (np.array(loss_train), np.array(accuracy_train)))\n",
    "    print(\"Loss valid: %3f, accuracy valid: %3f\" % (np.array(loss_valid), np.array(accuracy_valid)))\n",
    "\n",
    "    list_loss_train.append(np.array(loss_train))\n",
    "    list_loss_valid.append(np.array(loss_valid))\n",
    "    list_accuracy_train.append(np.array(accuracy_train))\n",
    "    list_accuracy_valid.append(np.array(accuracy_valid))\n",
    "\n",
    "    # Drawing loss, accuracy of train and valid\n",
    "    model.drawcurve(list_loss_train, list_loss_valid, 'loss_train', 'loss_valid')\n",
    "    model.drawcurve(list_accuracy_train, list_accuracy_valid, 'acc_train', 'acc_valid')\n",
    "\n",
    "\n",
    "# Evaluate model on test set\n",
    "loss, accuracy = model.evaluate(testX, testY)\n",
    "print(\"Loss valid: %3f, accuracy valid: %3f\" % (loss, accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
